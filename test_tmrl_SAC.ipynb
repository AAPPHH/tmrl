{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with SAC (rllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yann\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Yann\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Yann\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Yann\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Yann\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Yann\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Yann\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Yann\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Yann\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Yann\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Yann\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Yann\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import ray\n",
    "import ray.rllib.agents as agents\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "import gym.spaces as spaces\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part implements live plotting\n",
    "\n",
    "def live_plot(data_dict,\n",
    "              xlabel='',\n",
    "              figsize=(7,5),\n",
    "              title='',\n",
    "              save=False,\n",
    "              fname='figure.pdf',\n",
    "              erase=True):\n",
    "    if erase:\n",
    "        clear_output(wait=True)\n",
    "    plt.figure(figsize=figsize)\n",
    "    for label in data_dict:\n",
    "        plt.plot(data_dict[label][1], data_dict[label][0], label=label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.legend(loc='upper left') # the plot evolves to the right\n",
    "    if save:\n",
    "        plt.savefig(fname, bbox_inches='tight')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_mode: truncate_episodes\n",
      "callbacks: <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>\n",
      "clip_actions: true\n",
      "collect_metrics_timeout: 180\n",
      "compress_observations: false\n",
      "custom_resources_per_worker: {}\n",
      "eager: -1\n",
      "eager_tracing: false\n",
      "env_config: {}\n",
      "evaluation_config: {}\n",
      "evaluation_num_episodes: 10\n",
      "evaluation_num_workers: 0\n",
      "exploration_config:\n",
      "  type: StochasticSampling\n",
      "explore: true\n",
      "extra_python_environs_for_driver: {}\n",
      "extra_python_environs_for_worker: {}\n",
      "fake_sampler: false\n",
      "framework: tf\n",
      "gamma: 0.99\n",
      "ignore_worker_failures: false\n",
      "in_evaluation: false\n",
      "input: sampler\n",
      "input_evaluation:\n",
      "- is\n",
      "- wis\n",
      "local_tf_session_args:\n",
      "  inter_op_parallelism_threads: 8\n",
      "  intra_op_parallelism_threads: 8\n",
      "log_level: WARN\n",
      "log_sys_usage: true\n",
      "lr: 0.0001\n",
      "memory: 0\n",
      "memory_per_worker: 0\n",
      "metrics_smoothing_episodes: 100\n",
      "min_iter_time_s: 0\n",
      "model:\n",
      "  conv_activation: relu\n",
      "  conv_filters: null\n",
      "  custom_action_dist: null\n",
      "  custom_model: null\n",
      "  custom_model_config: {}\n",
      "  custom_options: -1\n",
      "  custom_preprocessor: null\n",
      "  dim: 84\n",
      "  fcnet_activation: tanh\n",
      "  fcnet_hiddens:\n",
      "  - 256\n",
      "  - 256\n",
      "  framestack: true\n",
      "  free_log_std: false\n",
      "  grayscale: false\n",
      "  lstm_cell_size: 256\n",
      "  lstm_use_prev_action_reward: false\n",
      "  max_seq_len: 20\n",
      "  no_final_linear: false\n",
      "  state_shape: null\n",
      "  use_lstm: false\n",
      "  vf_share_layers: true\n",
      "  zero_mean: true\n",
      "monitor: false\n",
      "multiagent:\n",
      "  observation_fn: null\n",
      "  policies: {}\n",
      "  policies_to_train: null\n",
      "  policy_mapping_fn: null\n",
      "no_done_at_end: false\n",
      "no_eager_on_workers: false\n",
      "normalize_actions: false\n",
      "num_cpus_for_driver: 1\n",
      "num_cpus_per_worker: 1\n",
      "num_envs_per_worker: 1\n",
      "num_gpus: 0\n",
      "num_gpus_per_worker: 0\n",
      "num_workers: 2\n",
      "object_store_memory: 0\n",
      "object_store_memory_per_worker: 0\n",
      "observation_filter: NoFilter\n",
      "optimizer: {}\n",
      "output_compress_columns:\n",
      "- obs\n",
      "- new_obs\n",
      "output_max_file_size: 67108864\n",
      "postprocess_inputs: false\n",
      "preprocessor_pref: deepmind\n",
      "remote_env_batch_wait_ms: 0\n",
      "remote_worker_envs: false\n",
      "rollout_fragment_length: 200\n",
      "sample_async: false\n",
      "sample_batch_size: -1\n",
      "shuffle_buffer_size: 0\n",
      "soft_horizon: false\n",
      "synchronize_filters: true\n",
      "tf_session_args:\n",
      "  allow_soft_placement: true\n",
      "  device_count:\n",
      "    CPU: 1\n",
      "  gpu_options:\n",
      "    allow_growth: true\n",
      "  inter_op_parallelism_threads: 2\n",
      "  intra_op_parallelism_threads: 2\n",
      "  log_device_placement: false\n",
      "timesteps_per_iteration: 0\n",
      "train_batch_size: 200\n",
      "use_pytorch: -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# These are the common rrlib settings, worth reading\n",
    "\n",
    "if True:\n",
    "    print(pretty_print(agents.trainer.COMMON_CONFIG.copy()))\n",
    "else:\n",
    "    COMMON_CONFIG = {\n",
    "    # === Debugging ===\n",
    "    # Whether to write episode stats and videos to the agent log dir\n",
    "    \"monitor\": False,\n",
    "    # Set the ray.rllib.* log level for the agent process and its workers.\n",
    "    # Should be one of DEBUG, INFO, WARN, or ERROR. The DEBUG level will also\n",
    "    # periodically print out summaries of relevant internal dataflow (this is\n",
    "    # also printed out once at startup at the INFO level).\n",
    "    \"log_level\": \"INFO\",\n",
    "    # Callbacks that will be run during various phases of training. These all\n",
    "    # take a single \"info\" dict as an argument. For episode callbacks, custom\n",
    "    # metrics can be attached to the episode by updating the episode object's\n",
    "    # custom metrics dict (see examples/custom_metrics_and_callbacks.py). You\n",
    "    # may also mutate the passed in batch data in your callback.\n",
    "    \"callbacks\": {\n",
    "        \"on_episode_start\": None,     # arg: {\"env\": .., \"episode\": ...}\n",
    "        \"on_episode_step\": None,      # arg: {\"env\": .., \"episode\": ...}\n",
    "        \"on_episode_end\": None,       # arg: {\"env\": .., \"episode\": ...}\n",
    "        \"on_sample_end\": None,        # arg: {\"samples\": .., \"worker\": ...}\n",
    "        \"on_train_result\": None,      # arg: {\"trainer\": ..., \"result\": ...}\n",
    "        \"on_postprocess_traj\": None,  # arg: {\n",
    "                                      #   \"agent_id\": ..., \"episode\": ...,\n",
    "                                      #   \"pre_batch\": (before processing),\n",
    "                                      #   \"post_batch\": (after processing),\n",
    "                                      #   \"all_pre_batches\": (other agent ids),\n",
    "                                      # }\n",
    "    },\n",
    "    # Whether to attempt to continue training if a worker crashes.\n",
    "    \"ignore_worker_failures\": False,\n",
    "    # Log system resource metrics to results.\n",
    "    \"log_sys_usage\": True,\n",
    "    # Enable TF eager execution (TF policies only).\n",
    "    \"eager\": False,\n",
    "    # Enable tracing in eager mode. This greatly improves performance, but\n",
    "    # makes it slightly harder to debug since Python code won't be evaluated\n",
    "    # after the initial eager pass.\n",
    "    \"eager_tracing\": False,\n",
    "    # Disable eager execution on workers (but allow it on the driver). This\n",
    "    # only has an effect is eager is enabled.\n",
    "    \"no_eager_on_workers\": False,\n",
    "\n",
    "    # === Policy ===\n",
    "    # Arguments to pass to model. See models/catalog.py for a full list of the\n",
    "    # available model options.\n",
    "    \"model\": MODEL_DEFAULTS,\n",
    "    # Arguments to pass to the policy optimizer. These vary by optimizer.\n",
    "    \"optimizer\": {},\n",
    "\n",
    "    # === Environment ===\n",
    "    # Discount factor of the MDP\n",
    "    \"gamma\": 0.99,\n",
    "    # Number of steps after which the episode is forced to terminate. Defaults\n",
    "    # to `env.spec.max_episode_steps` (if present) for Gym envs.\n",
    "    \"horizon\": None,\n",
    "    # Calculate rewards but don't reset the environment when the horizon is\n",
    "    # hit. This allows value estimation and RNN state to span across logical\n",
    "    # episodes denoted by horizon. This only has an effect if horizon != inf.\n",
    "    \"soft_horizon\": False,\n",
    "    # Don't set 'done' at the end of the episode. Note that you still need to\n",
    "    # set this if soft_horizon=True, unless your env is actually running\n",
    "    # forever without returning done=True.\n",
    "    \"no_done_at_end\": False,\n",
    "    # Arguments to pass to the env creator\n",
    "    \"env_config\": {},\n",
    "    # Environment name can also be passed via config\n",
    "    \"env\": None,\n",
    "    # Whether to clip rewards prior to experience postprocessing. Setting to\n",
    "    # None means clip for Atari only.\n",
    "    \"clip_rewards\": None,\n",
    "    # Whether to np.clip() actions to the action space low/high range spec.\n",
    "    \"clip_actions\": True,\n",
    "    # Whether to use rllib or deepmind preprocessors by default\n",
    "    \"preprocessor_pref\": \"deepmind\",\n",
    "    # The default learning rate\n",
    "    \"lr\": 0.0001,\n",
    "\n",
    "    # === Evaluation ===\n",
    "    # Evaluate with every `evaluation_interval` training iterations.\n",
    "    # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "    # Note that evaluation is currently not parallelized, and that for Ape-X\n",
    "    # metrics are already only reported for the lowest epsilon workers.\n",
    "    \"evaluation_interval\": None,\n",
    "    # Number of episodes to run per evaluation period.\n",
    "    \"evaluation_num_episodes\": 10,\n",
    "    # Extra arguments to pass to evaluation workers.\n",
    "    # Typical usage is to pass extra args to evaluation env creator\n",
    "    # and to disable exploration by computing deterministic actions\n",
    "    # TODO(kismuz): implement determ. actions and include relevant keys hints\n",
    "    \"evaluation_config\": {},\n",
    "\n",
    "    # === Resources ===\n",
    "    # Number of actors used for parallelism\n",
    "    \"num_workers\": 2,\n",
    "    # Number of GPUs to allocate to the trainer process. Note that not all\n",
    "    # algorithms can take advantage of trainer GPUs. This can be fractional\n",
    "    # (e.g., 0.3 GPUs).\n",
    "    \"num_gpus\": 0,\n",
    "    # Number of CPUs to allocate per worker.\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    # Number of GPUs to allocate per worker. This can be fractional.\n",
    "    \"num_gpus_per_worker\": 0,\n",
    "    # Any custom resources to allocate per worker.\n",
    "    \"custom_resources_per_worker\": {},\n",
    "    # Number of CPUs to allocate for the trainer. Note: this only takes effect\n",
    "    # when running in Tune.\n",
    "    \"num_cpus_for_driver\": 1,\n",
    "\n",
    "    # === Memory quota ===\n",
    "    # You can set these memory quotas to tell Ray to reserve memory for your\n",
    "    # training run. This guarantees predictable execution, but the tradeoff is\n",
    "    # if your workload exceeeds the memory quota it will fail.\n",
    "    # Heap memory to reserve for the trainer process (0 for unlimited). This\n",
    "    # can be large if your are using large train batches, replay buffers, etc.\n",
    "    \"memory\": 0,\n",
    "    # Object store memory to reserve for the trainer process. Being large\n",
    "    # enough to fit a few copies of the model weights should be sufficient.\n",
    "    # This is enabled by default since models are typically quite small.\n",
    "    \"object_store_memory\": 0,\n",
    "    # Heap memory to reserve for each worker. Should generally be small unless\n",
    "    # your environment is very heavyweight.\n",
    "    \"memory_per_worker\": 0,\n",
    "    # Object store memory to reserve for each worker. This only needs to be\n",
    "    # large enough to fit a few sample batches at a time. This is enabled\n",
    "    # by default since it almost never needs to be larger than ~200MB.\n",
    "    \"object_store_memory_per_worker\": 0,\n",
    "\n",
    "    # === Execution ===\n",
    "    # Number of environments to evaluate vectorwise per worker.\n",
    "    \"num_envs_per_worker\": 1,\n",
    "    # Default sample batch size (unroll length). Batches of this size are\n",
    "    # collected from workers until train_batch_size is met. When using\n",
    "    # multiple envs per worker, this is multiplied by num_envs_per_worker.\n",
    "    \"sample_batch_size\": 200,\n",
    "    # Training batch size, if applicable. Should be >= sample_batch_size.\n",
    "    # Samples batches will be concatenated together to this size for training.\n",
    "    \"train_batch_size\": 200,\n",
    "    # Whether to rollout \"complete_episodes\" or \"truncate_episodes\"\n",
    "    \"batch_mode\": \"truncate_episodes\",\n",
    "    # Use a background thread for sampling (slightly off-policy, usually not\n",
    "    # advisable to turn on unless your env specifically requires it)\n",
    "    \"sample_async\": False,\n",
    "    # Element-wise observation filter, either \"NoFilter\" or \"MeanStdFilter\"\n",
    "    \"observation_filter\": \"NoFilter\",\n",
    "    # Whether to synchronize the statistics of remote filters.\n",
    "    \"synchronize_filters\": True,\n",
    "    # Configure TF for single-process operation by default\n",
    "    \"tf_session_args\": {\n",
    "        # note: overriden by `local_tf_session_args`\n",
    "        \"intra_op_parallelism_threads\": 2,\n",
    "        \"inter_op_parallelism_threads\": 2,\n",
    "        \"gpu_options\": {\n",
    "            \"allow_growth\": True,\n",
    "        },\n",
    "        \"log_device_placement\": False,\n",
    "        \"device_count\": {\n",
    "            \"CPU\": 1\n",
    "        },\n",
    "        \"allow_soft_placement\": True,  # required by PPO multi-gpu\n",
    "    },\n",
    "    # Override the following tf session args on the local worker\n",
    "    \"local_tf_session_args\": {\n",
    "        # Allow a higher level of parallelism by default, but not unlimited\n",
    "        # since that can cause crashes with many concurrent drivers.\n",
    "        \"intra_op_parallelism_threads\": 8,\n",
    "        \"inter_op_parallelism_threads\": 8,\n",
    "    },\n",
    "    # Whether to LZ4 compress individual observations\n",
    "    \"compress_observations\": False,\n",
    "    # Wait for metric batches for at most this many seconds. Those that\n",
    "    # have not returned in time will be collected in the next iteration.\n",
    "    \"collect_metrics_timeout\": 180,\n",
    "    # Smooth metrics over this many episodes.\n",
    "    \"metrics_smoothing_episodes\": 100,\n",
    "    # If using num_envs_per_worker > 1, whether to create those new envs in\n",
    "    # remote processes instead of in the same worker. This adds overheads, but\n",
    "    # can make sense if your envs can take much time to step / reset\n",
    "    # (e.g., for StarCraft). Use this cautiously; overheads are significant.\n",
    "    \"remote_worker_envs\": False,\n",
    "    # Timeout that remote workers are waiting when polling environments.\n",
    "    # 0 (continue when at least one env is ready) is a reasonable default,\n",
    "    # but optimal value could be obtained by measuring your environment\n",
    "    # step / reset and model inference perf.\n",
    "    \"remote_env_batch_wait_ms\": 0,\n",
    "    # Minimum time per iteration\n",
    "    \"min_iter_time_s\": 0,\n",
    "    # Minimum env steps to optimize for per train call. This value does\n",
    "    # not affect learning, only the length of iterations.\n",
    "    \"timesteps_per_iteration\": 0,\n",
    "    # This argument, in conjunction with worker_index, sets the random seed of\n",
    "    # each worker, so that identically configured trials will have identical\n",
    "    # results. This makes experiments reproducible.\n",
    "    \"seed\": None,\n",
    "\n",
    "    # === Offline Datasets ===\n",
    "    # Specify how to generate experiences:\n",
    "    #  - \"sampler\": generate experiences via online simulation (default)\n",
    "    #  - a local directory or file glob expression (e.g., \"/tmp/*.json\")\n",
    "    #  - a list of individual file paths/URIs (e.g., [\"/tmp/1.json\",\n",
    "    #    \"s3://bucket/2.json\"])\n",
    "    #  - a dict with string keys and sampling probabilities as values (e.g.,\n",
    "    #    {\"sampler\": 0.4, \"/tmp/*.json\": 0.4, \"s3://bucket/expert.json\": 0.2}).\n",
    "    #  - a function that returns a rllib.offline.InputReader\n",
    "    \"input\": \"sampler\",\n",
    "    # Specify how to evaluate the current policy. This only has an effect when\n",
    "    # reading offline experiences. Available options:\n",
    "    #  - \"wis\": the weighted step-wise importance sampling estimator.\n",
    "    #  - \"is\": the step-wise importance sampling estimator.\n",
    "    #  - \"simulation\": run the environment in the background, but use\n",
    "    #    this data for evaluation only and not for learning.\n",
    "    \"input_evaluation\": [\"is\", \"wis\"],\n",
    "    # Whether to run postprocess_trajectory() on the trajectory fragments from\n",
    "    # offline inputs. Note that postprocessing will be done using the *current*\n",
    "    # policy, not the *behaviour* policy, which is typically undesirable for\n",
    "    # on-policy algorithms.\n",
    "    \"postprocess_inputs\": False,\n",
    "    # If positive, input batches will be shuffled via a sliding window buffer\n",
    "    # of this number of batches. Use this if the input data is not in random\n",
    "    # enough order. Input is delayed until the shuffle buffer is filled.\n",
    "    \"shuffle_buffer_size\": 0,\n",
    "    # Specify where experiences should be saved:\n",
    "    #  - None: don't save any experiences\n",
    "    #  - \"logdir\" to save to the agent log dir\n",
    "    #  - a path/URI to save to a custom output directory (e.g., \"s3://bucket/\")\n",
    "    #  - a function that returns a rllib.offline.OutputWriter\n",
    "    \"output\": None,\n",
    "    # What sample batch columns to LZ4 compress in the output data.\n",
    "    \"output_compress_columns\": [\"obs\", \"new_obs\"],\n",
    "    # Max output file size before rolling over to a new file.\n",
    "    \"output_max_file_size\": 64 * 1024 * 1024,\n",
    "\n",
    "    # === Multiagent ===\n",
    "    \"multiagent\": {\n",
    "        # Map from policy ids to tuples of (policy_cls, obs_space,\n",
    "        # act_space, config). See rollout_worker.py for more info.\n",
    "        \"policies\": {},\n",
    "        # Function mapping agent ids to policy ids.\n",
    "        \"policy_mapping_fn\": None,\n",
    "        # Optional whitelist of policies to train, or None for all policies.\n",
    "        \"policies_to_train\": None,\n",
    "    },}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_model:\n",
      "  fcnet_activation: relu\n",
      "  fcnet_hiddens:\n",
      "  - 256\n",
      "  - 256\n",
      "  hidden_activation: -1\n",
      "  hidden_layer_sizes: -1\n",
      "_deterministic_loss: false\n",
      "_use_beta_distribution: false\n",
      "batch_mode: truncate_episodes\n",
      "buffer_size: 1000000\n",
      "callbacks: <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>\n",
      "clip_actions: true\n",
      "collect_metrics_timeout: 180\n",
      "compress_observations: false\n",
      "custom_resources_per_worker: {}\n",
      "eager: -1\n",
      "eager_tracing: false\n",
      "env_config: {}\n",
      "evaluation_config: {}\n",
      "evaluation_num_episodes: 10\n",
      "evaluation_num_workers: 0\n",
      "exploration_config:\n",
      "  type: StochasticSampling\n",
      "explore: true\n",
      "extra_python_environs_for_driver: {}\n",
      "extra_python_environs_for_worker: {}\n",
      "fake_sampler: false\n",
      "final_prioritized_replay_beta: 0.4\n",
      "framework: tf\n",
      "gamma: 0.99\n",
      "grad_norm_clipping: -1\n",
      "ignore_worker_failures: false\n",
      "in_evaluation: false\n",
      "initial_alpha: 1.0\n",
      "input: sampler\n",
      "input_evaluation:\n",
      "- is\n",
      "- wis\n",
      "learning_starts: 1500\n",
      "local_tf_session_args:\n",
      "  inter_op_parallelism_threads: 8\n",
      "  intra_op_parallelism_threads: 8\n",
      "log_level: WARN\n",
      "log_sys_usage: true\n",
      "lr: 0.0001\n",
      "memory: 0\n",
      "memory_per_worker: 0\n",
      "metrics_smoothing_episodes: 100\n",
      "min_iter_time_s: 1\n",
      "model:\n",
      "  conv_activation: relu\n",
      "  conv_filters: null\n",
      "  custom_action_dist: null\n",
      "  custom_model: null\n",
      "  custom_model_config: {}\n",
      "  custom_options: -1\n",
      "  custom_preprocessor: null\n",
      "  dim: 84\n",
      "  fcnet_activation: tanh\n",
      "  fcnet_hiddens:\n",
      "  - 256\n",
      "  - 256\n",
      "  framestack: true\n",
      "  free_log_std: false\n",
      "  grayscale: false\n",
      "  lstm_cell_size: 256\n",
      "  lstm_use_prev_action_reward: false\n",
      "  max_seq_len: 20\n",
      "  no_final_linear: false\n",
      "  state_shape: null\n",
      "  use_lstm: false\n",
      "  vf_share_layers: true\n",
      "  zero_mean: true\n",
      "monitor: false\n",
      "multiagent:\n",
      "  observation_fn: null\n",
      "  policies: {}\n",
      "  policies_to_train: null\n",
      "  policy_mapping_fn: null\n",
      "n_step: 1\n",
      "no_done_at_end: false\n",
      "no_eager_on_workers: false\n",
      "normalize_actions: true\n",
      "num_cpus_for_driver: 1\n",
      "num_cpus_per_worker: 1\n",
      "num_envs_per_worker: 1\n",
      "num_gpus: 0\n",
      "num_gpus_per_worker: 0\n",
      "num_workers: 0\n",
      "object_store_memory: 0\n",
      "object_store_memory_per_worker: 0\n",
      "observation_filter: NoFilter\n",
      "optimization:\n",
      "  actor_learning_rate: 0.0003\n",
      "  critic_learning_rate: 0.0003\n",
      "  entropy_learning_rate: 0.0003\n",
      "optimizer: {}\n",
      "output_compress_columns:\n",
      "- obs\n",
      "- new_obs\n",
      "output_max_file_size: 67108864\n",
      "policy_model:\n",
      "  fcnet_activation: relu\n",
      "  fcnet_hiddens:\n",
      "  - 256\n",
      "  - 256\n",
      "  hidden_activation: -1\n",
      "  hidden_layer_sizes: -1\n",
      "postprocess_inputs: false\n",
      "preprocessor_pref: deepmind\n",
      "prioritized_replay: false\n",
      "prioritized_replay_alpha: 0.6\n",
      "prioritized_replay_beta: 0.4\n",
      "prioritized_replay_beta_annealing_timesteps: 20000\n",
      "prioritized_replay_eps: 1.0e-06\n",
      "remote_env_batch_wait_ms: 0\n",
      "remote_worker_envs: false\n",
      "rollout_fragment_length: 1\n",
      "sample_async: false\n",
      "sample_batch_size: -1\n",
      "shuffle_buffer_size: 0\n",
      "soft_horizon: false\n",
      "synchronize_filters: true\n",
      "target_entropy: auto\n",
      "target_network_update_freq: 0\n",
      "tau: 0.005\n",
      "tf_session_args:\n",
      "  allow_soft_placement: true\n",
      "  device_count:\n",
      "    CPU: 1\n",
      "  gpu_options:\n",
      "    allow_growth: true\n",
      "  inter_op_parallelism_threads: 2\n",
      "  intra_op_parallelism_threads: 2\n",
      "  log_device_placement: false\n",
      "timesteps_per_iteration: 100\n",
      "train_batch_size: 256\n",
      "twin_q: true\n",
      "use_pytorch: -1\n",
      "use_state_preprocessor: false\n",
      "worker_side_prioritization: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SAC default parameters, worth reading too:\n",
    "\n",
    "if True:\n",
    "    print(pretty_print(agents.sac.DEFAULT_CONFIG.copy()))\n",
    "else:\n",
    "    DEFAULT_CONFIG = with_common_config({\n",
    "    # === Model ===\n",
    "    \"twin_q\": True,\n",
    "    \"use_state_preprocessor\": False,\n",
    "    \"policy\": \"GaussianLatentSpacePolicy\",\n",
    "    # RLlib model options for the Q function\n",
    "    \"Q_model\": {\n",
    "        \"hidden_activation\": \"relu\",\n",
    "        \"hidden_layer_sizes\": (256, 256),\n",
    "    },\n",
    "    # RLlib model options for the policy function\n",
    "    \"policy_model\": {\n",
    "        \"hidden_activation\": \"relu\",\n",
    "        \"hidden_layer_sizes\": (256, 256),\n",
    "    },\n",
    "    # Unsquash actions to the upper and lower bounds of env's action space\n",
    "    \"normalize_actions\": True,\n",
    "\n",
    "    # === Learning ===\n",
    "    # Update the target by \\tau * policy + (1-\\tau) * target_policy\n",
    "    \"tau\": 5e-3,\n",
    "    # Target entropy lower bound. This is the inverse of reward scale,\n",
    "    # and will be optimized automatically.\n",
    "    \"target_entropy\": \"auto\",\n",
    "    # Disable setting done=True at end of episode.\n",
    "    \"no_done_at_end\": True,\n",
    "    # N-step target updates\n",
    "    \"n_step\": 1,\n",
    "\n",
    "    # === Evaluation ===\n",
    "    # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "    \"evaluation_interval\": 1,\n",
    "    # Number of episodes to run per evaluation period.\n",
    "    \"evaluation_num_episodes\": 1,\n",
    "    # Extra configuration that disables exploration.\n",
    "    \"evaluation_config\": {\n",
    "        \"exploration_enabled\": False,\n",
    "    },\n",
    "\n",
    "    # === Exploration ===\n",
    "    # Number of env steps to optimize for before returning\n",
    "    \"timesteps_per_iteration\": 100,\n",
    "    \"exploration_enabled\": True,\n",
    "\n",
    "    # === Replay buffer ===\n",
    "    # Size of the replay buffer. Note that if async_updates is set, then\n",
    "    # each worker will have a replay buffer of this size.\n",
    "    \"buffer_size\": int(1e6),\n",
    "    # If True prioritized replay buffer will be used.\n",
    "    # TODO(hartikainen): Make sure this works or remove the option.\n",
    "    \"prioritized_replay\": False,\n",
    "    \"prioritized_replay_alpha\": 0.6,\n",
    "    \"prioritized_replay_beta\": 0.4,\n",
    "    \"prioritized_replay_eps\": 1e-6,\n",
    "    \"prioritized_replay_beta_annealing_timesteps\": 20000,\n",
    "    \"final_prioritized_replay_beta\": 0.4,\n",
    "    \"compress_observations\": False,\n",
    "\n",
    "    # === Optimization ===\n",
    "    \"optimization\": {\n",
    "        \"actor_learning_rate\": 3e-4,\n",
    "        \"critic_learning_rate\": 3e-4,\n",
    "        \"entropy_learning_rate\": 3e-4,\n",
    "    },\n",
    "    # If not None, clip gradients during optimization at this value\n",
    "    \"grad_norm_clipping\": None,\n",
    "    # How many steps of the model to sample before learning starts.\n",
    "    \"learning_starts\": 1500,\n",
    "    # Update the replay buffer with this many samples at once. Note that this\n",
    "    # setting applies per-worker if num_workers > 1.\n",
    "    \"sample_batch_size\": 1,\n",
    "    # Size of a batched sampled from replay buffer for training. Note that\n",
    "    # if async_updates is set, then each worker returns gradients for a\n",
    "    # batch of this size.\n",
    "    \"train_batch_size\": 256,\n",
    "    # Update the target network every `target_network_update_freq` steps.\n",
    "    \"target_network_update_freq\": 0,\n",
    "\n",
    "    # === Parallelism ===\n",
    "    # Whether to use a GPU for local optimization.\n",
    "    \"num_gpus\": 0,\n",
    "    # Number of workers for collecting samples with. This only makes sense\n",
    "    # to increase if your environment is particularly slow to sample, or if\n",
    "    # you\"re using the Async or Ape-X optimizers.\n",
    "    \"num_workers\": 0,\n",
    "    # Whether to allocate GPUs for workers (if > 0).\n",
    "    \"num_gpus_per_worker\": 0,\n",
    "    # Whether to allocate CPUs for workers (if > 0).\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    # Whether to compute priorities on workers.\n",
    "    \"worker_side_prioritization\": False,\n",
    "    # Prevent iterations from going lower than this time span.\n",
    "    \"min_iter_time_s\": 1,\n",
    "\n",
    "    # DEPRECATED:\n",
    "    \"per_worker_exploration\": -1,\n",
    "    \"exploration_fraction\": -1,\n",
    "    \"schedule_max_timesteps\": -1,\n",
    "    \"exploration_initial_eps\": -1,\n",
    "    \"exploration_final_eps\": -1,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-11 18:54:57,138\tINFO resource_spec.py:212 -- Starting Ray with 9.28 GiB memory available for workers and up to 0.93 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-07-11 18:54:58,028\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.100',\n",
       " 'raylet_ip_address': '192.168.0.100',\n",
       " 'redis_address': '192.168.0.100:6379',\n",
       " 'object_store_address': 'tcp://127.0.0.1:55833',\n",
       " 'raylet_socket_name': 'tcp://127.0.0.1:59087',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': 'C:\\\\Users\\\\Yann\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2020-07-11_18-54-57_132772_11936'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-11 18:55:02,899\tWARNING worker.py:1047 -- The dashboard on node Skywalker failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Yann\\anaconda3\\lib\\site-packages\\ray\\dashboard/dashboard.py\", line 960, in <module>\n",
      "    metrics_export_address=metrics_export_address)\n",
      "  File \"C:\\Users\\Yann\\anaconda3\\lib\\site-packages\\ray\\dashboard/dashboard.py\", line 513, in __init__\n",
      "    build_dir = setup_static_dir(self.app)\n",
      "  File \"C:\\Users\\Yann\\anaconda3\\lib\\site-packages\\ray\\dashboard/dashboard.py\", line 414, in setup_static_dir\n",
      "    \"&& npm run build)\", build_dir)\n",
      "FileNotFoundError: [Errno 2] Dashboard build directory not found. If installing from source, please follow the additional steps required to build the dashboard(cd python/ray/dashboard/client && npm ci && npm run build): 'C:\\\\Users\\\\Yann\\\\anaconda3\\\\lib\\\\site-packages\\\\ray\\\\dashboard\\\\client/build'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This command starts ray.\n",
    "ray.init(num_gpus=1, object_store_memory=1000000000)  # can also be useful to set redis_max_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_TIME_STEPS_EPISODE = 100\n",
    "\n",
    "# environment:\n",
    "env_name = 'gym_tmrl:gym-tmrl-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policies definition:\n",
    "\n",
    "# these must be copy-pasted from the gym environement definition, \n",
    "# because we don't want to instantiate a simulator just to retrieve them:\n",
    "\n",
    "obs_space = spaces.Dict({\n",
    "    'objective_position': spaces.Box(low=-np.inf, high=np.inf, shape=(2,)),\n",
    "    'sensor_value': spaces.Box(low=-np.inf, high=np.inf, shape=(2,))  # vector toward the closest drone within the detection radius\n",
    "    })\n",
    "act_space = spaces.Box(low_bounds, high_bounds)\n",
    "\n",
    "policy_1 = (None,   # None means we use the default policy\n",
    "            obs_space,  # observation space\n",
    "            act_space,  # action space\n",
    "            {\"gamma\": 0.99})  # policy config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agent training: classic RL setting\n",
    "Now, we train our policy in the non-real-time setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration dictionnary for the gym environment:\n",
    "env_config = {\n",
    "    \"low_bounds\": low_bounds,\n",
    "    \"high_bounds\": high_bounds,\n",
    "    \"ep_max_length\": NB_TIME_STEPS_EPISODE,\n",
    "    \"drones_names\": ['d1', 'd2'],\n",
    "    # \"detection_radius\": DETECTION_RADIUS,\n",
    "    \"collision_radius\": COLLISION_RADIUS,\n",
    "    \"real_time\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(env_config):\n",
    "    return RllibMultiAgentEnv(env_name, config=env_config)\n",
    "\n",
    "register_env(\"my_multiagent_env\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rllib trainer config:\n",
    "\n",
    "config = agents.sac.DEFAULT_CONFIG.copy()\n",
    "\n",
    "# Heap memory to reserve for the trainer process (0 for unlimited). This\n",
    "# can be large if your are using large train batches, replay buffers, etc.\n",
    "config[\"memory\"] = 12000000000\n",
    "# Object store memory to reserve for the trainer process. Being large\n",
    "# enough to fit a few copies of the model weights should be sufficient.\n",
    "# This is enabled by default since models are typically quite small.\n",
    "config[\"object_store_memory\"] = 1000000000\n",
    "# Heap memory to reserve for each worker. Should generally be small unless\n",
    "# your environment is very heavyweight.\n",
    "config[\"memory_per_worker\"] = 0\n",
    "# Object store memory to reserve for each worker. This only needs to be\n",
    "# large enough to fit a few sample batches at a time. This is enabled\n",
    "# by default since it almost never needs to be larger than ~200MB.\n",
    "config[\"object_store_memory_per_worker\"] = 0\n",
    "\n",
    "config[\"num_gpus\"] = 1.0\n",
    "config[\"num_gpus_per_worker\"] = 0\n",
    "\n",
    "config[\"horizon\"] = 100\n",
    "config[\"timesteps_per_iteration\"] = 100\n",
    "config[\"train_batch_size\"] = 256\n",
    "\n",
    "config[\"normalize_actions\"] = False  # bug if set to true? default true\n",
    "config[\"buffer_size\"] = int(1e6)  # default 1e6\n",
    "\n",
    "config[\"learning_starts\"] = int(1e5) # default 1500 no effect?\n",
    "\n",
    "# config[\"batch_size\"] = ...\n",
    "# ...\n",
    "config[\"env_config\"] = env_config\n",
    "config[\"multiagent\"] = { # we can use several policies, each agent must be mapped to its policy\n",
    "                            \"policies\": {\n",
    "                                 # the first tuple value is None -> uses default policy\n",
    "                                 \"policy1\": policy_1\n",
    "                            },\n",
    "                            \"policy_mapping_fn\": lambda agent_id: \"policy1\"\n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# rllib trainer:\n",
    "trainer = agents.sac.SACTrainer(env=\"my_multiagent_env\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"/home/yann/ray_results/SAC_my_multiagent_env_2020-02-14_18-24-33jnyhtjgx/checkpoint_2578/checkpoint-2578\"\n",
    "trainer.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for live plotting:\n",
    "time_start = time.time()\n",
    "time_end = time_start\n",
    "idle_time = 0\n",
    "data_dict = {\n",
    "    'mean_reward':[[],[]],\n",
    "    'max_reward':[[],[]],\n",
    "    'min_reward':[[],[]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is to append live plotting:\n",
    "idle_time += time.time() - time_end\n",
    "\n",
    "# --- Training Algorithm: ---\n",
    "\n",
    "max_training_iterations = 10000\n",
    "\n",
    "# This is for early stopping:\n",
    "early_stopping_iterations = 1000\n",
    "\n",
    "max_avg_reward = -1.0 * np.inf\n",
    "best_iteration = -1\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for i in range(max_training_iterations):\n",
    "    result = trainer.train()  # trainer step\n",
    "    # early stopping:\n",
    "    early_stopping_counter += 1\n",
    "    if 'policy1' in result['policy_reward_mean']:\n",
    "        rew_mean = result['policy_reward_mean']['policy1']\n",
    "        if rew_mean > max_avg_reward:\n",
    "            max_avg_reward = rew_mean\n",
    "            best_iteration = i\n",
    "            early_stopping_counter = 0\n",
    "        timestamp = time.time() - time_start - idle_time\n",
    "        data_dict['mean_reward'][0].append(rew_mean)\n",
    "        data_dict['mean_reward'][1].append(timestamp)\n",
    "        data_dict['max_reward'][0].append(result['policy_reward_max']['policy1'])\n",
    "        data_dict['max_reward'][1].append(timestamp)\n",
    "        data_dict['min_reward'][0].append(result['policy_reward_min']['policy1'])\n",
    "        data_dict['min_reward'][1].append(timestamp)\n",
    "        live_plot(data_dict, xlabel='time (s)', figsize=(7,5), title='PPO multiagent training')\n",
    "        print(f\"iteration {i} - policy reward mean: {result['policy_reward_mean']} max: {result['policy_reward_max']}\")\n",
    "        print(pretty_print(result))\n",
    "    if early_stopping_counter >= early_stopping_iterations:\n",
    "        print(f\"The agent did not learn for the past {early_stopping_counter} iterations\")\n",
    "        break\n",
    "    # checkpoints:\n",
    "    if i % 100 == 0:\n",
    "        checkpoint = trainer.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)\n",
    "\n",
    "time_end = time.time()\n",
    "\n",
    "live_plot(data_dict, xlabel='time (s)', figsize=(7,5), title='SAC multiagent usual setting', save=False, fname='ppo1testsuite.pdf', erase=False)\n",
    "checkpoint = trainer.save()\n",
    "print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm may not have converged yet after many more iterations than in the single-agent setting! This is to be expected, as adding a second learning agent makes the environment much more difficult and non-Markov (we must study the effect of non-Markov environment in PPO). Also, the more the training progresses, and the more often the two drones collide in at least one episode. This needs further interpretation (check whether PPO has been implemented with the entropy term meant to ensure exploration).\n",
    "\n",
    "## Visual evaluation of the learnt policy\n",
    "\n",
    "Rllib allows to load a policy directly from a checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo 1 agent:\n",
    "checkpoint =\"/home/yann/ray_results/PPO_my_multiagent_env_2020-02-14_13-18-02j59jujiv/checkpoint_10/checkpoint-10\"\n",
    "trainer.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test this policy against its training setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRONE1_NAME = 'd1'\n",
    "DRONE2_NAME = 'd2'\n",
    "DRONE3_NAME = 'd3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config[\"drones_names\"] = [DRONE1_NAME]\n",
    "test_env = env_creator(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_n_init = test_env.reset()\n",
    "print(pretty_print(obs_n_init))\n",
    "test_env.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_n = obs_n_init\n",
    "hist_rew_n = {}\n",
    "for drone_name in env_config['drones_names']:\n",
    "        hist_rew_n[drone_name] = [0]\n",
    "done = False\n",
    "i = 0\n",
    "while done == False and i < 50:\n",
    "    actions = {}\n",
    "    for drone_name in env_config['drones_names']:\n",
    "        actions[drone_name] = trainer.compute_action(observation=obs_n[drone_name], policy_id='policy1')\n",
    "        print(\"DEBUG: action \", actions[drone_name])\n",
    "    obs_n, rew_n, done_n, _ = test_env.step(action_dict=actions)\n",
    "    for drone_name in env_config['drones_names']:\n",
    "        hist_rew_n[drone_name].append(rew_n[drone_name]+hist_rew_n[drone_name][-1])\n",
    "    done = done_n['__all__']\n",
    "    clear_output(wait=True)\n",
    "    test_env.env.render(datadict=hist_rew_n,\n",
    "                        xlabel=\"time step\",\n",
    "                        title=\"Cumulated rewards\",\n",
    "                        figsize=(10, 5),\n",
    "                        save=False,\n",
    "                        fname=f\"imgsav/ppo2train1test{i}.png\",\n",
    "                        dpi=200)\n",
    "    time.sleep(0.1)  # actually matplotlib is long enough to render the environment\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drone has learnt not to get too close to the objective, even if it sees no other drone!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config[\"drones_names\"] = [DRONE1_NAME, DRONE2_NAME, DRONE3_NAME]\n",
    "test_env = env_creator(env_config)\n",
    "\n",
    "obs_n_init = test_env.reset()\n",
    "print(pretty_print(obs_n_init))\n",
    "test_env.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_n = obs_n_init\n",
    "hist_rew_n = {}\n",
    "for drone_name in env_config['drones_names']:\n",
    "        hist_rew_n[drone_name] = [0]\n",
    "done = False\n",
    "i = 0\n",
    "while done == False and i < 100:\n",
    "    actions = {}\n",
    "    for drone_name in env_config['drones_names']:\n",
    "        actions[drone_name] = trainer.compute_action(observation=obs_n[drone_name], policy_id='policy1')\n",
    "        print(\"DEBUG: act \", actions[drone_name], \"obs \", obs_n[drone_name])\n",
    "    obs_n, rew_n, done_n, _ = test_env.step(action_dict=actions)\n",
    "    for drone_name in env_config['drones_names']:\n",
    "        hist_rew_n[drone_name].append(rew_n[drone_name]+hist_rew_n[drone_name][-1])\n",
    "    done = done_n['__all__']\n",
    "    clear_output(wait=True)\n",
    "    test_env.env.render(datadict=hist_rew_n,\n",
    "                        xlabel=\"time step\",\n",
    "                        title=\"Cumulated rewards\",\n",
    "                        figsize=(10, 5),\n",
    "                        save=True,\n",
    "                        fname=f\"imgsav/ppo2train2test{i}.png\",\n",
    "                        dpi=200)\n",
    "    time.sleep(0.1)  # actually matplotlib is long enough to render the environment\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dwo drones seem to have learnt a cooperative behavior in which the two of them get at specific positions at either side of the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
